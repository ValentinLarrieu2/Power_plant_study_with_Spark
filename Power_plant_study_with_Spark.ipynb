{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We import the data\n",
    "dfraw = spark.read.  \\\n",
    "         option(\"header\", \"true\"). \\\n",
    "         option(\"nullValue\", \"?\"). \\\n",
    "         option(\"inferSchema\", \"true\"). \\\n",
    "         option(\"sep\", \";\"). \\\n",
    "         csv(\"/FileStore/tables/powerDataForTP.csv\") \n",
    "\n",
    "df = spark.read.  \\\n",
    "         option(\"header\", \"true\"). \\\n",
    "         option(\"nullValue\", \"?\"). \\\n",
    "         option(\"inferSchema\", \"true\"). \\\n",
    "         option(\"sep\", \";\"). \\\n",
    "         csv(\"/FileStore/tables/powerDataForTP.csv\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg\n",
    "\n",
    "####We compute the mean of column and replace the missing value with it\n",
    "mean_dict = { col: 'mean' for col in df.columns }\n",
    "col_avgs = df.agg( mean_dict ).collect()[0].asDict()\n",
    "col_avgs = { k[4:-1]: v for k,v in col_avgs.iteritems() }\n",
    "dfmodified=df.fillna( col_avgs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(dfmodified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, VectorIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder,CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "vec= VectorAssembler(\n",
    "  inputCols= [\n",
    "  'AT',\n",
    "  'V',\n",
    "  'AP',\n",
    "  'RH'\n",
    "  ],\n",
    "  outputCol = 'features'\n",
    ")\n",
    "data= vec.transform(dfmodified)\n",
    "print(data)\n",
    "modelData = data.select('features','PE')\n",
    "trainData, testData = modelData.randomSplit([0.7,0.3])\n",
    "\n",
    "#Visualization of the variables\n",
    "modelData.describe().show()\n",
    "trainData.describe().show()\n",
    "testData.describe().show()\n",
    "\n",
    "#Model Creation\n",
    "lr= LinearRegression(labelCol='PE',featuresCol='features',regParam=0.01)\n",
    "\n",
    "#We fit the model\n",
    "lrModel=lr.fit(trainData)\n",
    "\n",
    "#we create a summary and display it\n",
    "summary=lrModel.summary\n",
    "summary.predictions.show(truncate=False)\n",
    "\n",
    "\n",
    "#Evaluator\n",
    "evaluator = RegressionEvaluator(labelCol=\"PE\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "#Estimator\n",
    "lr1= LinearRegression(labelCol='PE',featuresCol='features')\n",
    "lr2= LinearRegression(labelCol='PE',featuresCol='features')\n",
    "lr3= LinearRegression(labelCol='PE',featuresCol='features')\n",
    "\n",
    "# parameter grid\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "param_grid1 = ParamGridBuilder().\\\n",
    "    addGrid(lr1.regParam, [0, 0.2, 0.4,0.6, 0.8, 1]).\\\n",
    "    addGrid(lr1.elasticNetParam, [0]).\\\n",
    "    build()\n",
    "param_grid2 = ParamGridBuilder().\\\n",
    "    addGrid(lr2.regParam, [0, 0.2, 0.4,0.6, 0.8, 1]).\\\n",
    "    addGrid(lr2.elasticNetParam, [1]).\\\n",
    "    build()\n",
    "param_grid3 = ParamGridBuilder().\\\n",
    "    addGrid(lr2.regParam, [0, 0.2, 0.4,0.6, 0.8, 1]).\\\n",
    "    addGrid(lr2.elasticNetParam, [0.2, 0.4,0.6, 0.8]).\\\n",
    "    build()\n",
    "# cross-validation model\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "cv1 = CrossValidator(estimator=lr1, estimatorParamMaps=param_grid1, evaluator=evaluator, numFolds=4)\n",
    "cv2 = CrossValidator(estimator=lr2, estimatorParamMaps=param_grid2, evaluator=evaluator, numFolds=4)\n",
    "cv3 = CrossValidator(estimator=lr3, estimatorParamMaps=param_grid3, evaluator=evaluator, numFolds=4)\n",
    "\n",
    "#Fit cross-validation model\n",
    "cv_model1 = cv1.fit(trainData)\n",
    "cv_model2 = cv2.fit(trainData)\n",
    "cv_model3 = cv3.fit(trainData)\n",
    "\n",
    "#Prediction\n",
    "pred_training_cv1 = cv_model1.transform(trainData)\n",
    "pred_test_cv1 = cv_model1.transform(testData)\n",
    "\n",
    "pred_training_cv2 = cv_model2.transform(trainData)\n",
    "pred_test_cv2 = cv_model2.transform(testData)\n",
    "\n",
    "pred_training_cv3 = cv_model3.transform(trainData)\n",
    "pred_test_cv3 = cv_model3.transform(testData)\n",
    "\n",
    "#Evaluation\n",
    "# performance on training data\n",
    "print(\"Train data performance Lasso = \", evaluator.evaluate(pred_training_cv1))\n",
    "print(\"Train data performance Ridge = \", evaluator.evaluate(pred_training_cv2))\n",
    "print(\"Train data performance Elastic = \", evaluator.evaluate(pred_training_cv3))\n",
    "\n",
    "# performance on test data\n",
    "print(\"Test data performance Lasso = \", evaluator.evaluate(pred_test_cv1))\n",
    "print(\"Test data performance Ridge = \", evaluator.evaluate(pred_test_cv2))\n",
    "print(\"Test data performance Elastic = \", evaluator.evaluate(pred_test_cv3))\n",
    "\n",
    "#Intercept and coefficients\n",
    "print(\"Best intercept Lasso = \", cv_model1.bestModel.intercept)\n",
    "#print(\"Best coefficients Lasso = \", cv_model1.bestModel.coefficients)\n",
    "\n",
    "print(\"Best intercept Ridge = \", cv_model2.bestModel.intercept)\n",
    "#print(\"Best coefficients Ridge = \", cv_model2.bestModel.coefficients)\n",
    "\n",
    "print(\"Best intercept Elastic = \", cv_model3.bestModel.intercept)\n",
    "#print(\"Best coefficients Elastic = \", cv_model3.bestModel.coefficients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.ml.regression import DecisionTreeRegressor,RandomForestRegressor\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "trainData2, testData2 = dfmodified.randomSplit([0.7,0.3])\n",
    "\n",
    "dt = DecisionTreeRegressor(maxDepth=3)\n",
    "rf = RandomForestRegressor(numTrees=3, maxDepth=3)\n",
    "\n",
    "dt.setLabelCol(\"PE\")\\\n",
    "  .setPredictionCol(\"prediction\")\\\n",
    "  .setFeaturesCol(\"features\")\n",
    "\n",
    "rf.setLabelCol(\"PE\")\\\n",
    "  .setPredictionCol(\"prediction\")\\\n",
    "  .setFeaturesCol(\"features\")\n",
    "  \n",
    "#rf = RandomForestRegressor(labelCol=\"PE\", featuresCol=\"features\", predictionCol(\"prediction\"))\n",
    "\n",
    "# We create the Pipeline\n",
    "dtPipeline = Pipeline()\n",
    "rfPipeline = Pipeline()\n",
    "\n",
    "# We set the parameters of the pipeline\n",
    "dtPipeline.setStages([vec, dt])\n",
    "rfPipeline.setStages([vec, rf])\n",
    "\n",
    "eval = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"PE\", metricName=\"rmse\")\n",
    "crossval = CrossValidator(estimator=dtPipeline, evaluator=eval, numFolds=4)\n",
    "crossval2 = CrossValidator(estimator=rfPipeline, evaluator=eval, numFolds=4)\n",
    "crossval.setEstimator(dtPipeline)\n",
    "crossval2.setEstimator(rfPipeline)\n",
    "\n",
    "# We try different maxDepth\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(dt.maxDepth, [1,2,3,4,5])\n",
    "             .build())\n",
    "\n",
    "paramGrid2 = (ParamGridBuilder()\n",
    "             .addGrid(rf.maxBins, [20,40,60,80,100])\n",
    "             .build())\n",
    "\n",
    "# We add the grid\n",
    "crossval.setEstimatorParamMaps(paramGrid)\n",
    "crossval2.setEstimatorParamMaps(paramGrid2)\n",
    "\n",
    "# We generate the best model\n",
    "dtModel = crossval.fit(trainData2).bestModel\n",
    "rfModel = crossval2.fit(trainData2).bestModel\n",
    "\n",
    "#Prediction\n",
    "pred_training_cv3 = dtModel.transform(trainData2)\n",
    "pred_test_cv3 = dtModel.transform(testData2)\n",
    "\n",
    "pred_training_cv4 = rfModel.transform(trainData2)\n",
    "pred_test_cv4 = rfModel.transform(testData2)\n",
    "\n",
    "# performance on training data\n",
    "print(\"Train data performance Decision Tree = \", evaluator.evaluate(pred_training_cv3))\n",
    "print(\"Train data performance Random Forest = \", evaluator.evaluate(pred_training_cv4))\n",
    "\n",
    "# performance on test data\n",
    "print(\"Test data performance Decision Tree = \", evaluator.evaluate(pred_test_cv3))\n",
    "print(\"Test data performance Random Forest = \", evaluator.evaluate(pred_test_cv4))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "name": "TP4_Rendu_Grincourt_Larrieu",
  "notebookId": 3740699848577986
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
